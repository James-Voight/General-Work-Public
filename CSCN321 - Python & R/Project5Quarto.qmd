---
title: "EDA & Visualization Assignment"
author: "James Voight"
date: "09-22-2024"
date-format: long
format:
  #pdf
  #pptx
  #docx:
    #reference-doc: knowles-custom-reference-doc.docx # make sure it is in the same folder as the .qmd
      # Run this in your python terminal to create a basic reference-doc that you can modify:
      # quarto pandoc -o custom-reference-doc.docx --print-default-data-file reference.docx
  html: # or docx for Word Document
    toc: true # includes table of contents
    code-fold: true # option for collapsing code blocks in html
execute:
  echo: true # includes output
  warning: false # turns off warnings
  error: false # if set to true, then stops running at error
  output: true
python:
  version: "3.12.4"  # Specify your Python version here
  
---

# Python Code
```{r}
# install.packages("dplyr")
# install.packages("lubridate")
# install.packages("ggplot2")
# install.packages("factoextra")
# install.packages("dbscan")
# install.packages("tidyr")
```

## Q1a - Python
```{r}
# 1. Setup
ecommerce <- read.csv("C:/Users/james/Downloads/ecommerce2.csv")
View(ecommerce)

# Check for missing values
print("Missing values(before):")
print(colSums(is.na(ecommerce)))

# Handle missing values

# Generate unique identifiers for missing CustomerIDs
missing_ids <- is.na(ecommerce$CustomerID)
ecommerce$CustomerID[missing_ids] <- seq(from = max(ecommerce$CustomerID, na.rm = TRUE) + 1, 
                                    length.out = sum(missing_ids))

# Convert Age to numeric, replacing non-numeric values with NA
ecommerce$Age <- as.numeric(as.character(ecommerce$Age))

print("Missing values(after):")
print(colSums(is.na(ecommerce)))
View(ecommerce)

# Print dimensions
print(dim(ecommerce))
```


## Q2a - Python
```{r}
# 2. Create RFR ecommerceframe with one-hot encoded Gender
library(dplyr)
library(lubridate)
library(tidyr)  # For one-hot encoding

RFR <- ecommerce %>%
  group_by(CustomerID) %>%
  summarise(
    Age = max(Age, na.rm = TRUE),
    Gender = first(Gender),
    Num_Orders = n_distinct(OrderID),
    Total_Quantity = sum(Quantity),
    Recency = as.numeric(as.Date("2024-01-02") - max(as.Date(TransactionDate, format = "%m/%d/%y"))),
    Customer_Duration = as.numeric(max(as.Date(TransactionDate, format = "%m/%d/%y")) - min(as.Date(TransactionDate, format = "%m/%d/%y"))),
    Avg_Transaction = mean(Price * Quantity),
    Total_Spent = sum(Price * Quantity)
  ) %>%
  # One-hot encode Gender
  mutate(Gender = factor(Gender)) %>%
  pivot_wider(names_from = Gender, values_from = Gender, 
              values_fn = length, values_fill = 0,
              names_prefix = "Gender_")

View(RFR)
```

## Q3a - Python
```{r}
# 3.
# Print dimensions of RFR
print(dim(RFR))
# Create RFR_Master
RFR_Master <- RFR
```

## Q4a - Python
```{r}
# 4. Initial clustering (2 clusters)
library(factoextra)
set.seed(123)
kmeans_result <- kmeans(scale(RFR[, -c(1, 2)]), centers = 2)
clusterA <- kmeans_result$cluster

# Visualize initial clustering using fviz_cluster
print(fviz_cluster(kmeans_result, data = scale(RFR[, -c(1, 2)])) +
      labs(title = "Initial Clustering (2 Clusters)"))
```

## Q5a - Python
```{r}
# 5. Determine optimal number of clusters
library(factoextra)

# # Elbow method
print(fviz_nbclust(scale(RFR[, -c(1, 2)]), kmeans, method = "wss", nstart = 15))
# optimal = 4

# # Silhouette method
print(fviz_nbclust(scale(RFR[, -c(1, 2)]), kmeans, method = "silhouette", nstart = 15))
# optimal = 4

# Perform clustering with optimal number
kmeans_result_optimal <- kmeans(scale(RFR[, -c(1, 2)]), centers = 4, nstart = 25)
clusterB <- kmeans_result_optimal$cluster

print(fviz_cluster(kmeans_result_optimal, data = scale(RFR[, -c(1, 2)])) +
      labs(title = "Optimal Clustering (4 Clusters)"))


```

## Q6a - Python
```{r}
# 6. Outlier analysis using DBSCAN
library(dbscan)

#dbscan model
dbmodel <- dbscan(scale(RFR), eps = 1.3, minPts = 5)
clusterC <- dbmodel$cluster

# visualize
print(fviz_cluster(dbmodel, data = scale(RFR),
            show.clust.cent = TRUE,
            outlier.color = 'black',
            ellipse.type = 'confidence'))
```

## Q7a - Python
```{r}
# Add location for each customer
RFR_Master$Location <- ecommerce$Country[match(RFR_Master$CustomerID, ecommerce$CustomerID)]

# Print first 10 records
print(head(RFR_Master, 10))

```

## Q8a - Python
```{r}
# 8. Add cluster assignments to RFR_Master
RFR_Master$clusterA <- clusterA
RFR_Master$clusterB <- clusterB
RFR_Master$clusterC <- clusterC

# Print first 10 records
print(head(RFR_Master, 10))
```

## Q10a - Python
```{r}
# 10. Descriptive statistics for chosen clustering approach (e.g., clusterB)
cluster_summary <- RFR_Master %>%
  group_by(clusterB) %>%
  summarise(across(c(Age, Num_Orders, Total_Quantity, Recency, Customer_Duration, Avg_Transaction, Total_Spent),
                   list(mean = mean, median = median, sd = sd))) %>%
  ungroup() %>%
  mutate(Cluster = paste("Cluster", clusterB)) %>%
  select(Cluster, everything(), -clusterB)

View(cluster_summary)

# Export results
write.csv(cluster_summary, "C:/Users/james/Downloads/cluster_summary.csv", row.names = FALSE)
```

## Q11a - Python
```{r}
# 11. Principal Component Analysis
pca_result <- prcomp(scale(RFR[, -c(1, 2)]), center = TRUE, scale. = TRUE)

# Scree plot
print(fviz_eig(pca_result, addlabels = TRUE))

# Biplot
print(fviz_pca_biplot(pca_result, label = "var"))

# Determine number of dimensions for 75% variance
print(summary(pca_result))

# Visualize variable contributions
print(fviz_contrib(pca_result, choice = "var", axes = 1:2))
```

# Cluster Analysis Models

## Q1
First, we need to load the dataset and address any missing values. There were only 20 missing values in the CustomerID column. To address this, unique ID's were assigned. The Age column wasn't being registered as a numeric value so any non-numeric values were converted to NA before being handled. The dataset dimensions were then printed.

## Q2
The RFM (Recency, Frequency, Monetary) table was created through the grouping of CustomerID, Age, Gender, Num_Orders, Total_Quantity, Recency, Customer_Duration, Avg_Transaction, and Total_Spent. Gender was 'M' or 'F' so one-hot encoding was used to transform it into binary features (Gender_Female and Gender_Male). The RFM table was named RFR and is used for futher analysis in the code.

## Q3
The dimensions of RFR were printed and RFR_Master was created as a direct copy of the RFR table.

## Q4
By using the kmeans clustering algorithm, 2 initial clusters were used on the scaled ecommerce data. This was visualized using the fvzi_cluster function which highlighted the separation of the customers into 2 distinct segments.

## Q5
The next step was to find the optimal amount of clusters. To do this the elbow and silhouette methods were used. Based on the resulted outputs, it was deduced that 4 was the opitmal amount of clusters. The clustering prcess was then repeated with the new, updated parameter and then visualized.

## Q6
In order to find outliers in the data, the DBSCAN algorithm was used. This takes into account both the spatial proximity and density of the data points. Based on the visualization, I would again recommend 4 clusters. This analysis helps provided more insights into atypical customer behavior that may need attention (outliers).

## Q7
Customer locations were added to the dataset by matching CustomerID with the corresponding Country. This gives us the geographical context of the customer. The first 10 records were then printed.

## Q8
The results from the initial 2-cluster (clusterA), optimal 4-cluster (clusterB), and DBSCAN analyses (clusterC) were added as new columns to the RFR_Master table. The first 10 records were then printed again.

## Q9
Clusters A, B, and C were analyzed and compared through their visualizations and cluster assignments. The 2-cluster kmeans model (A) gave a very broad segmentations of customers, but lacked really any differentiating behaviors. The 4-cluster model (B) revealed much mor granular groupings, giving highlight to the different spending patterns, recency, and frequency. On the other hand, DBSCAN used density and spacing to identify clusters, which was effective in isolating outliers. However, it was not good in larger groups. At the end, the 4-cluster kmeans model (A) offered the best, interpretable results/segmentation, while DBSCAN was good at finding atypical behavior.

## Q10
A - Descriptive statistics were used for each variable in each cluster and then organized

B - Description of each segment:

Cluster 1 (Low value / Occassional customers): Lowest average number of orders, total quantity purchased, transaction value, and total spent. Also has the shortest average customer duration.
Cluster 2 (high value customers): Highest average total quantity purchased, transaction value, and total spent. Also has a high average number of orders and the second-longest customer duration.
Cluster 3 (medium value engaged customers): Highest average number of orders with a moderate average total quantity purchased, transaction value, and total spent. Also has the longest average customer duration.
Cluster 4 (medium value occassional customers): Low average number of orders and total quantity purchased. They have a short average custer duration and the second-lowest total spent. They do have the second-highest average transaction value.

C - The results were then exported to a cluster_summary.csv file.

## Q11
A - principal component analysis requires these features:

-Numeric variables
-linear relationships
-An adequate sample size (at least 10 per variable)
-Variables are of similar scale

B - Number of Dimensions for 75% Variance
From the Cumulative Proportion column:

TThe first 3 components cover 77.68% or the variance in the dataset. This meets the threshold for retaining sufficient information of at least 75% while lowering complexity. In the contribution graph, you can easily see that the Gender, total spent, and total quantity are the 3 main factors. The fact that there is a steep drop off in explained variance after the third feature/component (also shown in the scree plot) reinforces the choice to keep only these 3 dimensions.

C - The biplot and contribution bar chart show how each variable influences the principal components:

PC1 (32.4% Variance): Dominated by Total_Spent, Avg_Transaction, and Num_Orders, reflecting customer spending and transaction frequency.
PC2 (25.2% Variance): Dominated by Recency and Customer_Duration, indicating time-based behaviors such as loyalty and engagement levels.
PC3 (20.1% Variance): Dominated by Age and Total_Quantity, capturing demographic and purchasing behavior patterns.
These three dimensions collectively capture spending, time, and demographic trends, providing a simplified yet comprehensive view of the data.

Visualizations to Support Analysis
Scree Plot: Confirms that the first three components are sufficient to represent the data while minimizing redundancy.
Biplot: Shows the relationship between principal components, variables, and data points. Being able to showing clear variable groupings.
Contribution Bar Chart: Highlights the variables most responsible for each dimension. This aids in the interpretation of PC1, PC2, and PC3.

{{< pagebreak >}}

# References

###### Hendrickson Publishers. (2004). The holy Bible: King James Version.