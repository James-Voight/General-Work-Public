---
title: "EDA & Visualization Assignment"
author: "James Voight"
date: "09-22-2024"
date-format: long
format:
  #pdf
  #pptx
  #docx:
    #reference-doc: knowles-custom-reference-doc.docx # make sure it is in the same folder as the .qmd
      # Run this in your python terminal to create a basic reference-doc that you can modify:
      # quarto pandoc -o custom-reference-doc.docx --print-default-data-file reference.docx
  html: # or docx for Word Document
    toc: true # includes table of contents
    code-fold: true # option for collapsing code blocks in html
execute:
  echo: true # includes output
  warning: false # turns off warnings
  error: false # if set to true, then stops running at error
  output: true
python:
  version: "3.12.4"  # Specify your Python version here
  
---

# Python Code
```{python}
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve
import warnings

```

## Q1a - Python
```{python}
#1
# Load the dataset
path = "C:/Users/james/Downloads/"
campaign_data = pd.read_csv(path + "campaign_offer_rev-1.csv")
```


## Q2a - Python
```{python}
#2
# Coercing specific columns based on the actual dataset contents
# Convert 'CustID' to string
campaign_data['CustID'] = campaign_data['CustID'].astype(str)

# Convert 'Kidhome' and 'Teenhome' to binary numeric values (1 for 'Yes', 0 for 'No')
campaign_data['Kidhome'] = campaign_data['Kidhome'].map({'Yes': 1, 'No': 0})
campaign_data['Teenhome'] = campaign_data['Teenhome'].map({'Yes': 1, 'No': 0})

# Convert categorical columns to string type for consistent handling in encoding
categorical_columns = ['Marital_Status', 'Education']
campaign_data[categorical_columns] = campaign_data[categorical_columns].astype(str)

# Ensure other binary indicator columns are integers
binary_columns = ['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2', 'Complain', 'Response']
campaign_data[binary_columns] = campaign_data[binary_columns].astype(int)

# Ensure numerical columns are of type float for scaling purposes
numerical_columns = ['Income', 'Age', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 
                     'MntFishProducts', 'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases', 
                     'NumWebPurchases', 'NumAppPurchases', 'NumStorePurchases', 
                     'NumWebVisitsMonth', 'MntTotal', 'MntRegularProds', 'AcceptedCmpOverall']
campaign_data[numerical_columns] = campaign_data[numerical_columns].astype(float)
```

## Q1b - Python
```{python}
#1b
# Check for missing values
print("\nMissing values before imputation:")
print(campaign_data.isnull().sum())

# Handle missing values
numerical_columns_with_missing = ['Income', 'Age', 'Recency', 'Kidhome', 'Teenhome']
for col in numerical_columns_with_missing:
    campaign_data[col].fillna(campaign_data[col].median(), inplace=True)

campaign_data['AcceptedCmpOverall'].fillna(0, inplace=True)

print("\nMissing values after imputation:")
print(campaign_data.isnull().sum())
```

## Q3a - Python
```{python}
#3
# Feature selection
features = campaign_data.columns.drop(['CustID', 'Response'])
target = 'Response'

# Visualize target variable distribution
campaign_data['Response'] = campaign_data['Response'].astype('category')
campaign_data['Response'].value_counts().plot.bar()
plt.title('Distribution of Response')
plt.show()
```

## Q4a - Python
```{python}
#4
# Split the data into training and testing sets
X = campaign_data[features]
y = campaign_data[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)
print("Training set shape:", X_train.shape)
print("Testing set shape:", X_test.shape)
```

## Q5a - Python
```{python}
#5
# Prepare the data set for analysis
numeric_features = X.select_dtypes(include=['int64', 'float64']).columns
categorical_features = X.select_dtypes(include=['object']).columns

# Function to remove outliers
def remove_outliers(df, columns):
    for column in columns:
        Q1 = df[column].quantile(0.25)
        Q3 = df[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]
    return df

# Remove outliers from training data
X_train = remove_outliers(X_train, numeric_features)
y_train = y_train.loc[X_train.index]

print("Training set shape after removing outliers:", X_train.shape)

# Create preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ]), numeric_features),
        ('cat', Pipeline([
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
            ('onehot', OneHotEncoder(drop='first', sparse_output=False))
        ]), categorical_features)
    ])

```

## Q6a - Python
```{python}
#6
# Build the first model (Logistic Regression)
lr_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression(max_iter=10000, random_state=42))
])

lr_params = {
    'classifier__C': [0.1, 1, 10],
    'classifier__penalty': ['l2'],
    'classifier__solver': ['lbfgs', 'newton-cholesky', 'sag', 'saga']
}

print("Starting Logistic Regression Grid Search...")
lr_grid = GridSearchCV(lr_pipeline, lr_params, cv=20, scoring='accuracy', n_jobs=-1, error_score='raise')
try:
    lr_grid.fit(X_train, y_train)
    print("Logistic Regression Grid Search completed successfully.")
    print('Best Parameters for Logistic Regression:', lr_grid.best_params_)
    print('Best CV Score for Logistic Regression:', lr_grid.best_score_)
    
    model1 = lr_grid.best_estimator_
    y_pred_lr = model1.predict(X_test)
    lr_accuracy = accuracy_score(y_test, y_pred_lr)
    print("\nLogistic Regression Test Accuracy:", lr_accuracy)
except Exception as e:
    print(f"An error occurred during Logistic Regression fitting: {e}")
    print("Logistic Regression Grid Search failed. Proceeding with default Logistic Regression.")
    model1 = Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', LogisticRegression(max_iter=10000, random_state=42))
    ])
    model1.fit(X_train, y_train)
    y_pred_lr = model1.predict(X_test)
    lr_accuracy = accuracy_score(y_test, y_pred_lr)
    print("\nDefault Logistic Regression Test Accuracy:", lr_accuracy)

```

## Q7a - Python
```{python}
#7
# Build the second model (Random Forest)
rf_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

rf_params = {
    'classifier__n_estimators': [100, 200],
    'classifier__max_depth': [5, 10, None],
    'classifier__min_samples_split': [2, 5]
}

print("\nStarting Random Forest Grid Search...")
rf_grid = GridSearchCV(rf_pipeline, rf_params, cv=18, scoring='accuracy', n_jobs=-1, error_score='raise')
try:
    rf_grid.fit(X_train, y_train)
    print("Random Forest Grid Search completed successfully.")
    print('Best RF Parameters:', rf_grid.best_params_)
    print('Best RF CV Score:', rf_grid.best_score_)
    
    model2 = rf_grid.best_estimator_
    y_pred_rf = model2.predict(X_test)
    rf_accuracy = accuracy_score(y_test, y_pred_rf)
    print("\nRandom Forest Test Accuracy:", rf_accuracy)
except Exception as e:
    print(f"An error occurred during Random Forest fitting: {e}")
    print("Random Forest Grid Search failed. Proceeding with default Random Forest.")
    model2 = Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', RandomForestClassifier(random_state=42))
    ])
    model2.fit(X_train, y_train)
    y_pred_rf = model2.predict(X_test)
    rf_accuracy = accuracy_score(y_test, y_pred_rf)
    print("\nDefault Random Forest Test Accuracy:", rf_accuracy)
```

## Q8a - Python
```{python}
#8
# Compare the two models
print("\nModel Comparison:")
print(f"Logistic Regression Accuracy: {lr_accuracy}")
print(f"Random Forest Accuracy: {rf_accuracy}")

if rf_accuracy > lr_accuracy:
    print("\nRandom Forest is better")
    best_model = model2
    y_pred = y_pred_rf
else:
    print("\nLogistic Regression is better")
    best_model = model1
    y_pred = y_pred_lr
```

## Q9a - Python
```{python}
# 9. Create confusion matrix and classification report for the best model
cm = confusion_matrix(y_test, y_pred)
cr = classification_report(y_test, y_pred)

print("\nConfusion Matrix:")
print(cm)
print("\nClassification Report:")
print(cr)

# Visualize confusion matrix
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix - Best Model')
plt.colorbar()
tick_marks = np.arange(2)
plt.xticks(tick_marks, ['Did not respond', 'Responded'])
plt.yticks(tick_marks, ['Did not respond', 'Responded'])
plt.xlabel('Predicted label')
plt.ylabel('True label')

# Add text annotations to the confusion matrix
thresh = cm.max() / 2.
for i, j in np.ndindex(cm.shape):
    plt.text(j, i, format(cm[i, j], 'd'),
             horizontalalignment="center",
             color="white" if cm[i, j] > thresh else "black")

plt.tight_layout()
plt.show()

# ROC Curve
y_pred_proba = best_model.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
roc_auc = roc_auc_score(y_test, y_pred_proba)

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

print("\nAnalysis complete.")
```

# Precictive Models

## Q1 Data Loading and Missing Value Handling:
The data is loaded from a csv file called campaign_off_rev-1.csv where there were missing values in Income, Age, Recency, Kidhome, Teenhome, and AcceptedCmpOverall. The numerical columns, the mean value was used to impute any missing values to help preserve how the data is distributed overall. The AcceptedCmpOverall column's missing values were filled with 0 to help maintain the size of the data set. The coercion in part 2 was done after loading in the data set, but before handling the missing values.

## Q2 Feature Type Review and Coercion:
The customer ID column was converted to a string since it's an identifier. The columns with 'Yes' and 'No' values were mapped to the binary values 1 for 'Yes' and 0 for 'No' and categorical columns were made into a string type for consistent encoding. Numeric columns were also converted to float values to help with scaling the data.

## Q3 Feature Selection:
The two columns CustID and Response were excluded from being selected since CustID is an identfier (not predictive) and Response is the target variable. The visualization of the target variable distribution is used to comprehend class balance which is very important when interpreting model performance and accuracy. Having this initial feature set gives an analytical baseline to help refine the model.

## Q4 Data Splitting:
The data was split into 20% for testing and 80% for training using the function train_test_split. The random_state was set to the meaning of the universe (42) so that the model can be reproduced. This allows the split to have unbiased model evaluation. A large portion of the data is being used for training, yet still with a significant amount being kept out to test the ability of the model.

## Q5 Data Preparation and Preprocessing:
There are several steps that were used when preparing the data. Numerical features are identified for scaling while categorical for encoding. A function was also made to remove outliers by calculating the interquartile range (IQR) which was then applied the the training data, thus reducing the impact of outliers. StandardScaler was used for numerical features whiler categorical features used OneHotEncoder. This preprocessing approach confirms that all features are the same scale and that the categorical values are encoded properly.

## Q6 Building the First Model (Logistic Regression):
The first model chosen was Logistic Regression for its efficiency and interpretability. Grid search with cross-validation was used to find the ideal parameters such as regularization strength, penalty type, and solver algorithm. The model was evaluated using accuracy to give an unbiased estimate.

## Q7 Building the Second Model (Random Forest):
The second model chosen was Random Forest for its capability to capture complex/non-linear relationships 0f the data. Grid search was also utilized for this model with the same types of parameters. Random Forest often performs very good on various ranges of datasets and was also evaluated with the accuracy metric for consistency.

## Q8 Model Comparison:
Comparing the two models based on their accuracy with the test set, we were able to determine which model generalizes the data better. Overall, Random Forest performed better with an accuracy of about 82.5% suggesting that their are some complex, non-linear relationships within the data that the Logistic Regression model wan't able to capture well. The Logistic Regression model only had an accuracy of about 68.7%.

## Q9 Final Model Evaluation:
A confusion matrix and classification report were then made of the best performing model (in this case Random Forest). The model had 345 true negatives, 29 false positives, 48 false negatives, and 19 true positives. The model had an accuracy of 88% when predicting non-responders, but only 40% when identfying responders. The total number of acutal non-responders was 374 and the actual total number of responders was 67.

## Q10
Based on the confusion matrix and report, the model is good at predicting non-responders. However, it is not good at identifyng responders and has a large class imbalance (374 vs 67). While an accuracy of 82.5% is high and good, it is not the greatest. Some recommendation that I would make is to find out how to deal with the class imbalance (maybe with class weights) and to next focus on the models ability to predict responders if that is what the business needs.

A Roc curve was then created for fun to show the trade-off between the true positive rate and false positive rate.

## Biblical Worldview
As Christians, we can see predictive models as a tool that can help us understand and steward God's creation. They allow us to be able to make informed decisions based on analyzed data. In Matthew 25:14-30, Jesus taught the parable of talents. We were taught how important it was to wisely keep track of and manage the resources that were given to us. Predictive models greatly increase our efficiency in our management of resources in the current world. We can also consider how to use these tools to help our neighbor, trying to genuinely help a customer and meet their needs.

{{< pagebreak >}}

# References

###### Hendrickson Publishers. (2004). The holy Bible: King James Version.