{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee20bc4",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ebe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e53d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"C:/Users/james/Downloads/Amazon_reviews/Amazon_reviews.csv\"\n",
    "\n",
    "PRODUCT_COL = \"ProductId\"   # from your CSV\n",
    "REVIEW_COL = \"Text\"         # from your CSV\n",
    "\n",
    "RNN_CELL_TYPE = \"LSTM\"      # or \"GRU\"\n",
    "\n",
    "MODEL_PATH = \"amazon_rnn_model.keras\"\n",
    "TOKENIZER_PATH = \"amazon_tokenizer.pkl\"\n",
    "OUTPUT_EXCEL = \"C:/Users/james/Downloads/filtered_reviews_with_sentiment.xlsx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9092b2",
   "metadata": {},
   "source": [
    "Load Data and Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b534c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 568454\n",
      "Columns: ['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text']\n",
      "\n",
      "Top 5 products by review count:\n",
      "1. B007JFMH8M\n",
      "2. B002QWP8H0\n",
      "3. B002QWHJOU\n",
      "4. B002QWP89S\n",
      "5. B0026RQTGE\n",
      "\n",
      "Using target product: B007JFMH8M\n",
      "Rows for chosen product (before dedup): 913\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Drop rows without review text\n",
    "df = df.dropna(subset=[REVIEW_COL])\n",
    "\n",
    "print(\"Total rows:\", len(df))\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Top 5 products by number of reviews\n",
    "top_products = (\n",
    "    df[PRODUCT_COL]\n",
    "    .value_counts()\n",
    "    .nlargest(5)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(\"\\nTop 5 products by review count:\")\n",
    "for i, p in enumerate(top_products):\n",
    "    print(f\"{i+1}. {p}\")\n",
    "\n",
    "# ONE product to train on (changing index 0-4 for top 5 products)\n",
    "target_product = top_products[0]\n",
    "print(\"\\nUsing target product:\", target_product)\n",
    "\n",
    "df_product = df[df[PRODUCT_COL] == target_product].copy()\n",
    "print(\"Rows for chosen product (before dedup):\", len(df_product))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a27440",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6f1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_sequence_length = 40\n",
    "embedding_dim = 100\n",
    "units = 128\n",
    "vocab_size_limit = 10000\n",
    "batch_size = 64\n",
    "epochs = 100              # requirement; EarlyStopping will cut it short\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a139cd",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6cde7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using product: B007JFMH8M\n",
      "Unique reviews: 910\n",
      "['i love these cookies  not only are they healthy but they taste great and are so soft  i will definitely add these to my grocery list', 'quaker soft baked oatmeal cookies with raisins are a delicious treat great for anytime of day  for examplebr br at breakfast i had one with a large banana and a cup of coffee and felt id had a relatively healthy start to the daybr br the next day at lunch following a tuna sandwich i had one with a glass of milk and was satisfied enough to not need a snack before dinner at 630br br the following night after dinner i had one with the remainder of my glass of wine delicious and again didnt feel the need to snack later in the eveningbr br each cookie is individually packaged and their texture is soft and moist with just the right amount of sweetness natural flavors used in the making are cinnamon and all spice  these flavorings give the cookies a real oldfashioned homemade tastebr br nutritionally the cookies have 170 calories each 15g saturated fat 150 mg sodium and 12g sugar they also have 2g of protein and contain 25g of fiberbr br while the calorie count may seem a bit high for one cookie they are good sized and 1 cookie per serving is certainly enough to satisfybr br because of their great taste and texture kids will probably enjoy them alsobr br if you like oatmeal raisin cookies give these a try', 'i am usually not a huge fan of oatmeal cookies but these literally melt in your mouth they are so soft and tasty i would definitely recommend these to someone who loves oatmeal and even those like me who would probably pick a different flavor over this one']\n",
      "Actual vocab_size used: 3037\n",
      "X shape: (47213, 39)\n",
      "y shape: (47213,)\n"
     ]
    }
   ],
   "source": [
    "# Drop missing review text\n",
    "df = df.dropna(subset=[\"Text\"])\n",
    "\n",
    "# Pick top product by review count\n",
    "top_products = df[\"ProductId\"].value_counts().nlargest(5).index.tolist()\n",
    "target_product = top_products[0]\n",
    "print(\"Using product:\", target_product)\n",
    "\n",
    "df_product = df[df[\"ProductId\"] == target_product].copy()\n",
    "\n",
    "# Normalize and deduplicate reviews\n",
    "def normalize_for_dedup(text):\n",
    "    text = str(text).lower().strip()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "df_product[\"clean_review\"] = df_product[\"Text\"].apply(normalize_for_dedup)\n",
    "df_product = df_product.drop_duplicates(subset=[\"clean_review\"], keep=\"first\")\n",
    "\n",
    "# Save the cleaned reviews list for tokenizer\n",
    "reviews = df_product[\"clean_review\"].tolist()\n",
    "print(\"Unique reviews:\", len(reviews))\n",
    "print(reviews[:3])  # preview\n",
    "\n",
    "\n",
    "# tokenize the text\n",
    "tokenizer = Tokenizer(num_words=vocab_size_limit, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = min(vocab_size_limit, len(word_index) + 1)\n",
    "\n",
    "print(\"Actual vocab_size used:\", vocab_size)\n",
    "\n",
    "sequences = []\n",
    "\n",
    "for sentence in reviews:\n",
    "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    if len(token_list) < 2:\n",
    "        continue\n",
    "    for i in range(2, len(token_list) + 1):\n",
    "        n_gram_seq = token_list[:i]\n",
    "        n_gram_seq = n_gram_seq[-max_sequence_length:]\n",
    "        sequences.append(n_gram_seq)\n",
    "\n",
    "if not sequences:\n",
    "    raise ValueError(\"No sequences created. Check reviews/product choice.\")\n",
    "\n",
    "# pad to fixed length\n",
    "sequences = pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=max_sequence_length,\n",
    "    padding=\"pre\"\n",
    ")\n",
    "\n",
    "# split into inputs (all but last) and labels (last token)\n",
    "X = sequences[:, :-1]\n",
    "y = sequences[:, -1]   # integer class ids\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de9aa23",
   "metadata": {},
   "source": [
    "Create/Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4c92c1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_15 (\u001b[38;5;33mEmbedding\u001b[0m)        │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_15 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.0488 - loss: 5.8547\n",
      "Epoch 2/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.1164 - loss: 5.1494\n",
      "Epoch 3/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.1641 - loss: 4.6757\n",
      "Epoch 4/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.1924 - loss: 4.3941\n",
      "Epoch 5/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.2136 - loss: 4.1795\n",
      "Epoch 6/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.2311 - loss: 4.0014\n",
      "Epoch 7/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.2429 - loss: 3.8471\n",
      "Epoch 8/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.2554 - loss: 3.7110\n",
      "Epoch 9/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.2653 - loss: 3.5910\n",
      "Epoch 10/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.2748 - loss: 3.4814\n",
      "Epoch 11/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 26ms/step - accuracy: 0.2848 - loss: 3.3749\n",
      "Epoch 12/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - accuracy: 0.2945 - loss: 3.2755\n",
      "Epoch 13/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.3059 - loss: 3.1824\n",
      "Epoch 14/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 29ms/step - accuracy: 0.3181 - loss: 3.0963\n",
      "Epoch 15/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.3313 - loss: 3.0136\n",
      "Epoch 16/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.3425 - loss: 2.9358\n",
      "Epoch 17/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.3544 - loss: 2.8639\n",
      "Epoch 18/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.3662 - loss: 2.7970\n",
      "Epoch 19/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.3754 - loss: 2.7368\n",
      "Epoch 20/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 30ms/step - accuracy: 0.3864 - loss: 2.6768\n",
      "Epoch 21/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 32ms/step - accuracy: 0.3952 - loss: 2.6216\n",
      "Epoch 22/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 35ms/step - accuracy: 0.4040 - loss: 2.5696\n",
      "Epoch 23/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.4135 - loss: 2.5168\n",
      "Epoch 24/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 30ms/step - accuracy: 0.4223 - loss: 2.4695\n",
      "Epoch 25/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 38ms/step - accuracy: 0.4312 - loss: 2.4207\n",
      "Epoch 26/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.4393 - loss: 2.3760\n",
      "Epoch 27/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.4481 - loss: 2.3311\n",
      "Epoch 28/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 39ms/step - accuracy: 0.4571 - loss: 2.2877\n",
      "Epoch 29/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.4637 - loss: 2.2472\n",
      "Epoch 30/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.4726 - loss: 2.2071\n",
      "Epoch 31/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 37ms/step - accuracy: 0.4811 - loss: 2.1683\n",
      "Epoch 32/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 36ms/step - accuracy: 0.4897 - loss: 2.1284\n",
      "Epoch 33/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 39ms/step - accuracy: 0.4981 - loss: 2.0930\n",
      "Epoch 34/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.5042 - loss: 2.0590\n",
      "Epoch 35/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.5086 - loss: 2.0268\n",
      "Epoch 36/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 29ms/step - accuracy: 0.5176 - loss: 1.9938\n",
      "Epoch 37/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 33ms/step - accuracy: 0.5229 - loss: 1.9648\n",
      "Epoch 38/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.5301 - loss: 1.9352\n",
      "Epoch 39/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.5371 - loss: 1.9031\n",
      "Epoch 40/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 33ms/step - accuracy: 0.5436 - loss: 1.8713\n",
      "Epoch 41/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.5516 - loss: 1.8408\n",
      "Epoch 42/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.5582 - loss: 1.8101\n",
      "Epoch 43/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.5636 - loss: 1.7833\n",
      "Epoch 44/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 30ms/step - accuracy: 0.5705 - loss: 1.7575\n",
      "Epoch 45/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.5774 - loss: 1.7285\n",
      "Epoch 46/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.5807 - loss: 1.7067\n",
      "Epoch 47/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.5862 - loss: 1.6798\n",
      "Epoch 48/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.5939 - loss: 1.6507\n",
      "Epoch 49/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.5960 - loss: 1.6332\n",
      "Epoch 50/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.6040 - loss: 1.6056\n",
      "Epoch 51/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.6068 - loss: 1.5868\n",
      "Epoch 52/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 30ms/step - accuracy: 0.6109 - loss: 1.5678\n",
      "Epoch 53/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.6171 - loss: 1.5460\n",
      "Epoch 54/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.6228 - loss: 1.5243\n",
      "Epoch 55/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 30ms/step - accuracy: 0.6240 - loss: 1.5132\n",
      "Epoch 56/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.6299 - loss: 1.4889\n",
      "Epoch 57/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.6322 - loss: 1.4745\n",
      "Epoch 58/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 30ms/step - accuracy: 0.6383 - loss: 1.4519\n",
      "Epoch 59/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.6443 - loss: 1.4317\n",
      "Epoch 60/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.6486 - loss: 1.4137\n",
      "Epoch 61/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.6511 - loss: 1.3952\n",
      "Epoch 62/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 28ms/step - accuracy: 0.6572 - loss: 1.3778\n",
      "Epoch 63/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.6599 - loss: 1.3688\n",
      "Epoch 64/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - accuracy: 0.6641 - loss: 1.3475\n",
      "Epoch 65/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - accuracy: 0.6689 - loss: 1.3340\n",
      "Epoch 66/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - accuracy: 0.6716 - loss: 1.3172\n",
      "Epoch 67/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - accuracy: 0.6785 - loss: 1.2963\n",
      "Epoch 68/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - accuracy: 0.6725 - loss: 1.3089\n",
      "Epoch 69/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - accuracy: 0.6806 - loss: 1.2830\n",
      "Epoch 70/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - accuracy: 0.6842 - loss: 1.2663\n",
      "Epoch 71/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - accuracy: 0.6829 - loss: 1.2638\n",
      "Epoch 72/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - accuracy: 0.6888 - loss: 1.2436\n",
      "Epoch 73/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 27ms/step - accuracy: 0.6974 - loss: 1.2177\n",
      "Epoch 74/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - accuracy: 0.6994 - loss: 1.2052\n",
      "Epoch 75/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 33ms/step - accuracy: 0.7016 - loss: 1.1911\n",
      "Epoch 76/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.7076 - loss: 1.1769\n",
      "Epoch 77/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.7081 - loss: 1.1668\n",
      "Epoch 78/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.7088 - loss: 1.1577\n",
      "Epoch 79/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.7126 - loss: 1.1470\n",
      "Epoch 80/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.7151 - loss: 1.1354\n",
      "Epoch 81/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.7168 - loss: 1.1260\n",
      "Epoch 82/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 30ms/step - accuracy: 0.7225 - loss: 1.1059\n",
      "Epoch 83/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 36ms/step - accuracy: 0.7221 - loss: 1.1077\n",
      "Epoch 84/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 36ms/step - accuracy: 0.7258 - loss: 1.0924\n",
      "Epoch 85/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 33ms/step - accuracy: 0.7277 - loss: 1.0870\n",
      "Epoch 86/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 36ms/step - accuracy: 0.7291 - loss: 1.0738\n",
      "Epoch 87/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 33ms/step - accuracy: 0.7337 - loss: 1.0597\n",
      "Epoch 88/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 30ms/step - accuracy: 0.7313 - loss: 1.0630\n",
      "Epoch 89/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 33ms/step - accuracy: 0.7385 - loss: 1.0413\n",
      "Epoch 90/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.7398 - loss: 1.0332\n",
      "Epoch 91/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 30ms/step - accuracy: 0.7458 - loss: 1.0171\n",
      "Epoch 92/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.7459 - loss: 1.0116\n",
      "Epoch 93/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 34ms/step - accuracy: 0.7427 - loss: 1.0189\n",
      "Epoch 94/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.7523 - loss: 0.9872\n",
      "Epoch 95/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 29ms/step - accuracy: 0.7526 - loss: 0.9850\n",
      "Epoch 96/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 33ms/step - accuracy: 0.7525 - loss: 0.9826\n",
      "Epoch 97/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.7551 - loss: 0.9690\n",
      "Epoch 98/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 31ms/step - accuracy: 0.7620 - loss: 0.9509\n",
      "Epoch 99/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 33ms/step - accuracy: 0.7627 - loss: 0.9452\n",
      "Epoch 100/100\n",
      "\u001b[1m738/738\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 29ms/step - accuracy: 0.7534 - loss: 0.9678\n",
      "Final training accuracy: 0.7534365653991699\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=max_sequence_length - 1),\n",
    "    # choose ONE: LSTM or GRU\n",
    "    LSTM(units),\n",
    "    # GRU(units),\n",
    "    Dense(vocab_size, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# a bit higher LR helps converge faster on this tiny model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"accuracy\",\n",
    "    patience=5,\n",
    "    min_delta=0.001,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X,\n",
    "    y,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "final_acc = history.history[\"accuracy\"][-1]\n",
    "print(\"Final training accuracy:\", final_acc)\n",
    "\n",
    "if final_acc < 0.70:\n",
    "    print(\"WARNING: accuracy < 70%. You can:\")\n",
    "    print(\"- increase units to 64\")\n",
    "    print(\"- lower learning rate to 0.002\")\n",
    "    print(\"- increase batch_size for stability/speed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b1c55",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1823f142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: amazon_rnn_model.keras\n",
      "Saved tokenizer to: amazon_tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "model.save(MODEL_PATH)\n",
    "print(f\"Saved model to: {MODEL_PATH}\")\n",
    "\n",
    "with open(TOKENIZER_PATH, \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(f\"Saved tokenizer to: {TOKENIZER_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ec05d",
   "metadata": {},
   "source": [
    "Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87811601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded model. Input length: 39\n",
      "Max sequence length (for generation): 40\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model(MODEL_PATH)\n",
    "\n",
    "with open(TOKENIZER_PATH, \"rb\") as f:\n",
    "    loaded_tokenizer = pickle.load(f)\n",
    "\n",
    "input_length = loaded_model.input_shape[1]   # max_sequence_len - 1\n",
    "max_sequence_len_loaded = input_length + 1\n",
    "\n",
    "print(\"Reloaded model. Input length:\", input_length)\n",
    "print(\"Max sequence length (for generation):\", max_sequence_len_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df599c6",
   "metadata": {},
   "source": [
    "Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "4bc53545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Text Generation Outputs ===\n",
      "1) This product was very good but i was pleasantly surprised how soft\n",
      "2) These Quaker Soft Baked Oatmeal Raisin Cookies are my favorite\n",
      "3) I think this cookie was in my\n"
     ]
    }
   ],
   "source": [
    "def generate_text(seed_text, next_words, tokenizer, model, max_sequence_length):\n",
    "    text = seed_text\n",
    "\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "        token_list = pad_sequences(\n",
    "            [token_list],\n",
    "            maxlen=max_sequence_length - 1,\n",
    "            padding=\"pre\"\n",
    "        )\n",
    "\n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        predicted_index = np.argmax(predicted_probs, axis=-1)[0]\n",
    "        next_word = tokenizer.index_word.get(predicted_index, \"\")\n",
    "\n",
    "        if not next_word:\n",
    "            break\n",
    "\n",
    "        text += \" \" + next_word\n",
    "\n",
    "    return text\n",
    "\n",
    "# Text Generation Runs\n",
    "\n",
    "print(\"=== Text Generation Outputs ===\")\n",
    "\n",
    "out1 = generate_text(\n",
    "    seed_text=\"This product\",\n",
    "    next_words=10,\n",
    "    tokenizer=loaded_tokenizer,\n",
    "    model=loaded_model,\n",
    "    max_sequence_length=max_sequence_len_loaded\n",
    ")\n",
    "print(\"1)\", out1)\n",
    "\n",
    "# \"These [product]\" — simple version using first token of product name\n",
    "product_label = \"Quaker Soft Baked Oatmeal Raisin Cookies\"\n",
    "seed2 = f\"These {product_label}\"\n",
    "out2 = generate_text(\n",
    "    seed_text=seed2,\n",
    "    next_words=3,\n",
    "    tokenizer=loaded_tokenizer,\n",
    "    model=loaded_model,\n",
    "    max_sequence_length=max_sequence_len_loaded\n",
    ")\n",
    "print(\"2)\", out2)\n",
    "\n",
    "out3 = generate_text(\n",
    "    seed_text=\"I think\",\n",
    "    next_words=5,\n",
    "    tokenizer=loaded_tokenizer,\n",
    "    model=loaded_model,\n",
    "    max_sequence_length=max_sequence_len_loaded\n",
    ")\n",
    "print(\"3)\", out3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f902ee5",
   "metadata": {},
   "source": [
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "dda5e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8fa856b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Excel with sentiment scores to: C:/Users/james/Downloads/filtered_reviews_with_sentiment.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis (VADER + TextBlob) & Excel Export\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def clean_for_sentiment(text: str) -> str:\n",
    "    return str(text).strip()\n",
    "\n",
    "vader_scores = []\n",
    "tb_polarities = []\n",
    "\n",
    "for review in df_product[REVIEW_COL]:\n",
    "    txt = clean_for_sentiment(review)\n",
    "\n",
    "    vs = analyzer.polarity_scores(txt)[\"compound\"]\n",
    "    vader_scores.append(vs)\n",
    "\n",
    "    tb = TextBlob(txt).sentiment.polarity\n",
    "    tb_polarities.append(tb)\n",
    "\n",
    "df_product[\"VADER_Compound\"] = vader_scores\n",
    "df_product[\"TextBlob_Polarity\"] = tb_polarities\n",
    "\n",
    "# Save Excel file with filtered reviews + sentiment scores\n",
    "df_product.to_excel(OUTPUT_EXCEL, index=False)\n",
    "print(f\"Saved Excel with sentiment scores to: {OUTPUT_EXCEL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae7f5b2",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb469536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Transformer Summarization ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary (English):\n",
      "Quaker Soft Baked Oatmeal Cookies with raisins are a delicious treat, great for anytime of day. Nutritionally, the cookies have 170 calories each, 1.5g saturated fat, 150 mg sodium, and 12\n",
      "=== Translation EN -> ES ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary (Spanish):\n",
      "Las galletas de avena al horno suave con pasas son una deliciosa delicia, ideal para cualquier momento del día. Nutricionalmente, las galletas tienen 170 calorías cada una, 1,5 g de grasa saturada, 150 mg de sodio y 12\n"
     ]
    }
   ],
   "source": [
    "# Summarization\n",
    "print(\"=== Transformer Summarization ===\")\n",
    "\n",
    "# Concatenate reviews (truncate to keep it reasonable for the model)\n",
    "all_reviews_text = \" \".join(df_product[REVIEW_COL].astype(str).tolist())\n",
    "max_chars = 4000\n",
    "if len(all_reviews_text) > max_chars:\n",
    "    all_reviews_text = all_reviews_text[:max_chars]\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\"\n",
    ")\n",
    "\n",
    "summary = summarizer(\n",
    "    all_reviews_text,\n",
    "    max_length=50,    # target about 30-50 words\n",
    "    min_length=30,\n",
    "    do_sample=False\n",
    ")[0][\"summary_text\"]\n",
    "\n",
    "print(\"Summary (English):\")\n",
    "print(summary)\n",
    "\n",
    "\n",
    "# Translation (EN -> ES)\n",
    "\n",
    "print(\"=== Translation EN -> ES ===\")\n",
    "\n",
    "translator = pipeline(\n",
    "    \"translation_en_to_es\",\n",
    "    model=\"Helsinki-NLP/opus-mt-en-es\"\n",
    ")\n",
    "\n",
    "translation = translator(summary)[0][\"translation_text\"]\n",
    "\n",
    "print(\"Summary (Spanish):\")\n",
    "print(translation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
